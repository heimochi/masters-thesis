---
title: "Analysis Grid Search and Feature Importance"
author: "MochiBear.Hei"
date: "04.06.2025"
format:
  html:
    toc: true
    toc-location: left
    embed-resources: true
    page-layout: full
    html-math-method: katex
editor: source
---

```{css, echo = F}
body{
  font-family: Helvetica;
  font-size: 16pt;
  max-width: 1000px;
  margin: auto;
  margin-left:310px;
}
pre{
  font-size: 14px;
}
/* Headers */
h1{
    font-size: 24pt;
  }
h1,h2{
    font-size: 20pt;
  }
h3,h4,h5,h6{
  font-size: 18pt;
}
#TOC {
  position: fixed;
  left: 0;
  top: 0;
  width: 300px;
  height: 100%;
  overflow:auto;
}
```

```{r setup,suppressPackageStartupMessages= TRUE}
#| include: false
#| message: false
#| warning: false
#| results: hide
knitr::opts_chunk$set(echo = TRUE, dpi = 300)

# Data manipulation
library(dplyr)
library(tidyr)

#models
library(caret)
library(xgboost)

# Plotting
library(ggplot2)
library(viridis)      

# Utilities
library(knitr)


```

#load
```{r}
##list of people to include in analysis analysis_subids.csv
subids <- read.csv("/Users/maggieheimvik/Desktop/GitHub/masters_thesis/Data/ocd/subids.csv") 
```

mri
```{r}
mri <- read.csv("/Users/maggieheimvik/Desktop/GitHub/masters_thesis/Data/ocd/mri.csv") %>%
  select (-smri_vol_scs_lesionlh, -smri_vol_scs_lesionrh, -smri_vol_scs_intracranialv) #removed these columns because they have 4897 NA each


full_dat <- mri[mri$src_subject_id %in% subids$src_subject_id, ]

```

bpm
```{r}
bpm <- read.csv("/Users/maggieheimvik/Desktop/GitHub/masters_thesis/Data/core/mental-health/mh_y_bpm.csv") %>%
  filter(eventname == "2_year_follow_up_y_arm_1") %>%
  select(src_subject_id, bpm_y_scr_internal_t, bpm_y_scr_external_t) %>%
  distinct(src_subject_id, .keep_all = TRUE) %>% #remove duplicates
  mutate(across(c(bpm_y_scr_internal_t, bpm_y_scr_external_t), ~ as.numeric(scale(.)))) #standardize

# merge
full_dat <- inner_join(full_dat, bpm, by = "src_subject_id")
```

cbcl
```{r}
cbcl <- read.csv("/Users/maggieheimvik/Desktop/LCBC/Data/ABCD/core/mental-health/mh_p_cbcl.csv") %>%
  filter(eventname == "2_year_follow_up_y_arm_1") %>%
  select(
    src_subject_id,cbcl_scr_syn_internal_t, cbcl_scr_syn_external_t
  ) %>%
  distinct(src_subject_id, .keep_all = TRUE) %>% # remove duplicates
  mutate(across(c(cbcl_scr_syn_internal_t, cbcl_scr_syn_external_t), ~ as.numeric(scale(.)))) # standardize scores

# merge
full_dat <- inner_join(full_dat, cbcl, by = "src_subject_id")
```

save for demogrpahics
-----------------

full_dat <- full_dat %>%
  select(
    src_subject_id
  )
write.csv(full_dat, "/Users/maggieheimvik/Desktop/GitHub/masters_thesis/Data/ocd/demo_subid.csv", row.names = FALSE)

-------------------

# missingness
```{r}
na_counts <- colSums(is.na(full_dat))
na_counts[na_counts > 0]  # print only columns with NA values


# mean imputation: 
full_dat <- full_dat %>%
  mutate(across(c(mrisdp_508, mrisdp_527, mrisdp_567, mrisdp_582, mrisdp_601, mrisdp_602, mrisdp_603, mrisdp_604),
                ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

```

# split data: test and training
```{r}
set.seed(123)  

# Split the data into training and test sets (70/30)
n <- nrow(full_dat)
train_indices <- sample(1:n, size = round(0.7 * n))

train_data <- full_dat[train_indices, ]  #4559 of 200
test_data <- full_dat[-train_indices, ] #1954 of 200
```


## xg(cbcl_internal)
```{r}
set.seed(1)

# Define target and predictors
target <- "cbcl_scr_syn_internal_t"
predictors <- colnames(full_dat)[!colnames(full_dat) %in% c("src_subject_id", "bpm_y_scr_internal_t", "bpm_y_scr_external_t", "cbcl_scr_syn_external_t", target)]

# Remove the target variable from the feature set
X_train <- train_data[, predictors, drop = FALSE]

# Target variable needs to be a vector
cbcl_scr_syn_internal_t_vector <- as.vector(train_data[[target]])

# Convert the data to a DMatrix object for XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = cbcl_scr_syn_internal_t_vector)

# Prepare test data
X_test <- test_data[, predictors, drop = FALSE]
y_test <- as.vector(test_data[[target]])

# Convert the test data to a DMatrix object
dtest <- xgb.DMatrix(data = as.matrix(X_test), label = y_test)

# Define the parameter grid for hyperparameter tuning
param_grid <- expand.grid(
  nrounds = c(50, 100),       # Number of boosting rounds
  max_depth = c(2, 7),        # Maximum depth of a tree
  min_child_weight = c(1, 2), # Minimum sum of instance weight (hessian) needed in a child
  eta = c(0.05, 0.1),         # Learning rate
  gamma = c(0.1, 1),          # Minimum loss reduction
  subsample = c(0.6, 0.8),    # Subsample ratio of training instances
  colsample_bytree = c(0.6, 0.8) # Subsample ratio of columns when constructing each tree
)

# Display the structure to confirm multiple combinations
str(param_grid)

# Set up cross-validation controls for regression
train_control <- trainControl(
  method = "cv",              # Use cross-validation
  number = 5,                 # 5-fold cross-validation
  verboseIter = TRUE          # Print training progress
)

# Train the model
model_cbcl_i <- train(
  x = X_train,                     # Training features
  y = cbcl_scr_syn_internal_t_vector, # Target variable
  method = "xgbTree",              # Use XGBoost for regression
  tuneGrid = param_grid,           # Expanded parameter grid for tuning
  trControl = train_control,       # Train control setup
  metric = "RMSE"                  # Evaluation metric for regression
)

# Print the best tuning parameters found
print(model_cbcl_i$bestTune)



```

### plot
#### hyperparameter tuning
```{r}


results_df <- model_cbcl_i$results
print(head(results_df))

results_df$model <- "CBCL"
write.csv(results_df, "results_cbcl_i.csv", row.names = FALSE)


########################
# Create the two plots first
plot1 <- ggplot(results_df, aes(x = factor(max_depth), y = RMSE, 
                                group = factor(min_child_weight), 
                                color = factor(min_child_weight))) +
  geom_point(size = 3) +
  geom_line(size = 1) +
  labs(
    title = "Model Complexity vs. Prediction Error",
    x = "Maximum Tree Depth",
    y = "Cross-Validation RMSE",
    color = "Regularization Strength"
  ) +
  theme_classic(base_size = 20) +
  scale_color_manual(
    values = c("1" = "#5ab4ac", "2" = "#d53e4f"),
    labels = c("Low", "Moderate")
  ) +
  theme(legend.position = "bottom")

plot2 <- ggplot(results_df, aes(x = nrounds, y = RMSE, 
                                color = factor(eta), 
                                linetype = factor(eta))) +
  geom_point(size = 3) +
  geom_line(linewidth = 1) +
  labs(
    title = "Model Performance Across Boosting Rounds",
    x = "Number of Boosting Rounds",
    y = "Cross-Validation RMSE",
    color = "Learning Speed",
    linetype = "Learning Speed"
  ) +
  theme_classic(base_size = 20) +
  scale_color_manual(
    values = c("0.05" = "#66c2a5", "0.1" = "#f46d43"),
    labels = c("Slow", "Fast")
  ) +
  scale_linetype_manual(
    values = c("0.05" = "dashed", "0.1" = "solid"),
    labels = c("Slow", "Fast")
  ) +
  theme(legend.position = "bottom")

# Arrange them side-by-side
combined_plot <- grid.arrange(plot1, plot2, ncol = 2)

ggsave("cbcl_combined_grid_plot.pdf",
       plot = combined_plot,    # careful: ggsave doesn't work directly on grid.arrange output
       width = 16,
       height = 6,
       dpi = 300)


```


#### carets post resample evaluation
```{r}
# Make predictions with the trained model on the test data
preds <- predict(model_cbcl_i, newdata = X_test)

# Evaluate using caret's postResample function
evaluation <- postResample(preds, y_test)

# Print evaluation results
print(evaluation)
```

#### feature imporance 
```{r}
# Final fitted model
final_model <- model_cbcl_i$finalModel

# Extract feature importance
importance_matrix <- xgb.importance(model = final_model)

# Create a renaming dictionary
rename_features <- c(
  "smri_vol_scs_ccat" =  "Corpus Callosum (Anterior)",
  "smri_vol_scs_bstem" = "Brainstem",
  "mrisdp_503" =         "L Anterior Transverse\n Collateral Sulcus",
  "mrisdp_563" =         "R Planum Temporale",
  "mrisdp_484" =         "L Gyrus Rectus",
  "mrisdp_549" =         "R Lingual Gyrus",
  "mrisdp_558" =         "R Gyrus Rectus",
  "mrisdp_568" =         "R Posterior Ramus of\n Lateral Sulcus",
  "mrisdp_494" =         "L Posterior Ramus of\n Lateral Sulcus",
  "mrisdp_590" =         "R Medial Orbital Sulcus",
  "mrisdp_584" =         "R Middle Occipital &\n Lunatus Sulcus"
)

# Replace feature names safely
importance_matrix$Feature <- ifelse(
  importance_matrix$Feature %in% names(rename_features),
  rename_features[importance_matrix$Feature],
  importance_matrix$Feature # Keep original name if no match
)

# Now plot the feature importance
p <- xgb.plot.importance(
  importance_matrix,
  top_n = 10,
  xlab = "Feature Importance",
  main = "Important Features in Predicting CBCL Internalizing"
)

#################

# Get importance
importance_matrix <- xgb.importance(model = xgb_model)

#write.csv(importance_matrix, "/Users/maggieheimvik/Desktop/GitHub/masters_thesis/Data/ocd/importance_cbcl_i.csv", row.names = FALSE)



# Top 10 important features
importance_top10 <- importance_matrix[1:10, ]

importance_top10$Feature <- factor(importance_top10$Feature, levels = importance_top10$Feature[order(importance_top10$Gain)])

# Plot manually with custom colors
ggplot(importance_top10, aes(x = reorder(Feature, Gain), y = Gain, fill = Feature)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Important Features in Predicting\n CBCL Internalizing",
    x = "Brain Region",
    y = "Feature Importance (Gain)"
  ) +
  theme_classic(base_size = 14) +
  scale_fill_brewer(palette = "Paired") +
  theme(legend.position = "none")  # hide legend

ggsave("important_features.pdf",
       plot = last_plot(),   # or just omit, it uses last plot automatically
       width = 8,
       height = 6)
```

## xg(bpm_internal)
```{r}
set.seed(1)

target <- "bpm_y_scr_internal_t"  
predictors <- colnames(full_dat)[!colnames(full_dat) %in% c("src_subject_id","cbcl_scr_syn_internal_t", "bpm_y_scr_external_t",  "cbcl_scr_syn_external_t", target)]  

set.seed(1)

X_train <- train_data[, predictors, drop = FALSE]

bpm_y_scr_internal_t_vector <- as.vector(train_data[[target]])

#make matrix
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = bpm_y_scr_internal_t_vector)

# Prepare test data
X_test <- test_data[, predictors, drop = FALSE]
y_test <- as.vector(test_data[[target]])

# Convert the test data to a DMatrix object
dtest <- xgb.DMatrix(data = as.matrix(X_test), label = y_test)


param_grid <- expand.grid(
  nrounds = c(50, 100),       
  max_depth = c(2, 7),        
  min_child_weight = c(1, 2), 
  eta = c(0.05, 0.1),         
  gamma = c(0.1, 1),          
  subsample = c(0.6, 0.8),    
  colsample_bytree = c(0.6, 0.8) 
)

#confirm
str(param_grid)

# CV controls
train_control <- trainControl(
  method = "cv",              
  number = 5,                 
  verboseIter = TRUE          
)

# Train the model
model_bpm_i <- train(
  x = X_train,                     
  y = bpm_y_scr_internal_t_vector, 
  method = "xgbTree",              
  tuneGrid = param_grid,           
  trControl = train_control,       
  metric = "RMSE"                  
)

# print the best 
print(model_bpm_i$bestTune)



```

```{r}


results_df <- model_bpm_i$results
print(head(results_df))

results_df$model <- "BPM"
write.csv(results_df, "results_bpm_i.csv", row.names = FALSE)


########################
# Create the two plots first
plot1 <- ggplot(results_df, aes(x = factor(max_depth), y = RMSE, 
                                group = factor(min_child_weight), 
                                color = factor(min_child_weight))) +
  geom_point(size = 3) +
  geom_line(size = 1) +
  labs(
    title = "Model Complexity vs. Prediction Error",
    x = "Maximum Tree Depth",
    y = "Cross-Validation RMSE",
    color = "Regularization Strength"
  ) +
  theme_classic(base_size = 20) +
  scale_color_manual(
    values = c("1" = "#5ab4ac", "2" = "#d53e4f"),
    labels = c("Low", "Moderate")
  ) +
  theme(legend.position = "bottom")

plot2 <- ggplot(results_df, aes(x = nrounds, y = RMSE, 
                                color = factor(eta), 
                                linetype = factor(eta))) +
  geom_point(size = 3) +
  geom_line(linewidth = 1) +
  labs(
    title = "Model Performance Across Boosting Rounds",
    x = "Number of Boosting Rounds",
    y = "Cross-Validation RMSE",
    color = "Learning Speed",
    linetype = "Learning Speed"
  ) +
  theme_classic(base_size = 20) +
  scale_color_manual(
    values = c("0.05" = "#66c2a5", "0.1" = "#f46d43"),
    labels = c("Slow", "Fast")
  ) +
  scale_linetype_manual(
    values = c("0.05" = "dashed", "0.1" = "solid"),
    labels = c("Slow", "Fast")
  ) +
  theme(legend.position = "bottom")

# Arrange them side-by-side
combined_plot <- grid.arrange(plot1, plot2, ncol = 2)

ggsave("bpm_combined_grid_plot.pdf",
       plot = combined_plot,    # careful: ggsave doesn't work directly on grid.arrange output
       width = 16,
       height = 6,
       dpi = 300)


```

Evaluate using carets post resample function evaluation
```{r}
# Make predictions with the trained model on the test data
preds <- predict(model_bpm_i, newdata = X_test)

# Evaluate using caret's postResample function
evaluation <- postResample(preds, y_test)

# Print evaluation results
print(evaluation)
```

#### feature importance
```{r}

# Final fitted model
final_model <- model_bpm_i$finalModel

# Extract feature importance
importance_matrix <- xgb.importance(model = final_model)

xgb.plot.importance(importance_matrix, 
                    top_n = 10,  # Show top 10 features
                    xlab = "Relative Importance")

# Create a renaming dictionary
rename_features <- c(
  "smri_vol_scs_ccat" = "Corpus Callosum(Anterior)",
  "mrisdp_495" = "L Occipital Pole",
  "smri_vol_scs_csf" = "Cerebrospinal Fluid",
  "mrisdp_505" = "L Inf Frontal Sulcus",
  "mrisdp_466" = "L Inf Frontal Gyrus\n(Orbital)",
  "mrisdp_465" = "L Inf Frontal Gyrus\n(Opercular)",
  "mrisdp_507" = "L Sup Frontal Sulcus",
  "mrisdp_533" = "R Ant Cingulate",
  "mrisdp_580" = "R Mid Frontal Sulcus",
  "mrisdp_528" = "R Fronto-Marginal Gyrus"
)



#write.csv(importance_matrix, "/Users/maggieheimvik/Desktop/GitHub/masters_thesis/Data/ocd/importance_bpm_i.csv", row.names = FALSE)


# Replace feature names safely
importance_matrix$Feature <- ifelse(
  importance_matrix$Feature %in% names(rename_features),
  rename_features[importance_matrix$Feature],
  importance_matrix$Feature # Keep original name if no match
)

# Now plot the feature importance
p <- xgb.plot.importance(
  importance_matrix,
  top_n = 10,
  xlab = "Feature Importance",
  main = "Important Features in Predicting BPM Internalizing"
)

#################



# Top 10 important features
importance_top10 <- importance_matrix[1:10, ]

importance_top10$Feature <- factor(importance_top10$Feature, levels = importance_top10$Feature[order(importance_top10$Gain)])

# Plot manually with custom colors
ggplot(importance_top10, aes(x = reorder(Feature, Gain), y = Gain, fill = Feature)) +
  geom_col(fill = "#add8e6") +
  coord_flip() +
  labs(
    title = "Important Features in Predicting\n BPM Internalizing",
    x = "Brain Region",
    y = "Feature Importance (Gain)"
  ) +
  theme_classic(base_size = 14) 



ggsave("important_features_bpm.pdf",
       plot = last_plot(),   # or just omit, it uses last plot automatically
       width = 8,
       height = 6)





```


