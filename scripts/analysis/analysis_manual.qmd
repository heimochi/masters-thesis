---
title: "Analysis"
author: "MochiBear.Hei"
date: "03.20.2025"
format:
  html:
    toc: true
    toc-location: left
    embed-resources: true
    page-layout: full
    html-math-method: katex
editor: source
---

```{css, echo = F}
body{
  font-family: Helvetica;
  font-size: 16pt;
  max-width: 1000px;
  margin: auto;
  margin-left:310px;
}
pre{
  font-size: 14px;
}
/* Headers */
h1{
    font-size: 24pt;
  }
h1,h2{
    font-size: 20pt;
  }
h3,h4,h5,h6{
  font-size: 18pt;
}
#TOC {
  position: fixed;
  left: 0;
  top: 0;
  width: 300px;
  height: 100%;
  overflow:auto;
}
```


```{r setup,suppressPackageStartupMessages= TRUE}
#| include: false
#| message: false
#| warning: false
#| results: hide
knitr::opts_chunk$set(echo = TRUE, dpi = 300)

# Data manipulation
library(dplyr)
library(tidyr)

#models
library(caret)
library(xgboost)

# Plotting
library(ggplot2)

# Utilities
library(knitr)
```

# load
```{r}
subids <- read.csv("/Users/maggieheimvik/Desktop/GitHub/masters_thesis/Data/ocd/src_subject_id.csv")

mri <- read.csv("/Users/maggieheimvik/Desktop/GitHub/masters_thesis/Data/ocd/mri.csv") %>%
  select (-smri_vol_scs_lesionlh, -smri_vol_scs_lesionrh) #removed these columns because they have 4897 NA each

#merge
full_dat <- mri[mri$src_subject_id %in% subids$src_subject_id, ]
```

```{r}
bpm <- read.csv("/Users/maggieheimvik/Desktop/GitHub/masters_thesis/Data/core/mental-health/mh_y_bpm.csv") %>%
  filter(eventname == "2_year_follow_up_y_arm_1") %>%
  select(src_subject_id, bpm_y_scr_internal_t, bpm_y_scr_external_t, bpm_y_scr_totalprob_t) %>%
  distinct(src_subject_id, .keep_all = TRUE) %>% #remove duplicates
  mutate(across(c(bpm_y_scr_internal_t, bpm_y_scr_external_t, bpm_y_scr_totalprob_t), ~ as.numeric(scale(.)))) #standardize

# merge
full_dat <- inner_join(full_dat, bpm, by = "src_subject_id")
```

CBCL Internalizing, Externalizing, and Total Problem scales have a full T-score range with a mean = 50 and SD = 10
```{r}
cbcl <- read.csv("/Users/maggieheimvik/Desktop/LCBC/Data/ABCD/core/mental-health/mh_p_cbcl.csv") %>%
  filter(eventname == "2_year_follow_up_y_arm_1") %>%
  select(
    src_subject_id,cbcl_scr_syn_internal_t, cbcl_scr_syn_external_t, cbcl_scr_syn_totprob_t
  ) %>%
  distinct(src_subject_id, .keep_all = TRUE) %>% # remove duplicates
  mutate(across(c(cbcl_scr_syn_internal_t, cbcl_scr_syn_external_t, cbcl_scr_syn_totprob_t), ~ as.numeric(scale(.)))) # standardize scores

# merge
full_dat <- inner_join(full_dat, cbcl, by = "src_subject_id")
```



-------- include demo?
demo <- read.csv("/Users/maggieheimvik/Desktop/GitHub/masters_thesis//Data/core/abcd-general/abcd_p_demo.csv") %>%
  filter(eventname == "baseline_year_1_arm_1") %>%
  select(src_subject_id, demo_sex_v2, race_ethnicity, demo_brthdat_v2)

# merge
full_dat <- inner_join(full_dat, demo, by = "src_subject_id")

--------


-------- extract ids for demographics 

full_dat <- full_dat %>%
  select(
    src_subject_id,
  )
write.csv(full_dat, "/Users/maggieheimvik/Desktop/GitHub/masters_thesis/Data/ocd/full.csv", row.names = FALSE)

--------

missingness
```{r}
na_counts <- colSums(is.na(full_dat))
na_counts[na_counts > 0]  # print only columns with NA values


# mean imputation: 
full_dat <- full_dat %>%
  mutate(across(c(mrisdp_508, mrisdp_527,    mrisdp_567, mrisdp_582, mrisdp_601, mrisdp_602, mrisdp_603, mrisdp_604,
                  bpm_y_scr_internal_t, bpm_y_scr_external_t, bpm_y_scr_totalprob_t),
                ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

```

# Split data: test and training
```{r}
set.seed(123)  

# Split the data into training and test sets (70/30)
n <- nrow(full_dat)
train_indices <- sample(1:n, size = round(0.7 * n))

train_data <- full_dat[train_indices, ]  #4897 of 205
test_data <- full_dat[-train_indices, ] #2099 of 205
```

#lm
## lm(bpm_internal ~ mri)
```{r}
lm_model <- lm(bpm_y_scr_internal_t ~ 
               mrisdp_454 + mrisdp_455 + mrisdp_456 + mrisdp_457 + mrisdp_458 + mrisdp_459 + 
               mrisdp_460 + mrisdp_461 + mrisdp_462 + mrisdp_463 + mrisdp_464 +
               mrisdp_465 + mrisdp_466 + mrisdp_467 + mrisdp_468 + mrisdp_469 + 
               mrisdp_470 + mrisdp_471 + mrisdp_472 + mrisdp_473 + mrisdp_474 +
               mrisdp_475 + mrisdp_476 + mrisdp_477 + mrisdp_478 + mrisdp_479 + 
               mrisdp_480 + mrisdp_481 + mrisdp_482 + mrisdp_483 + mrisdp_484 +
               mrisdp_485 + mrisdp_486 + mrisdp_487 + mrisdp_488 + mrisdp_489 + 
               mrisdp_490 + mrisdp_491 + mrisdp_492 + mrisdp_493 + mrisdp_494 +
               mrisdp_495 + mrisdp_496 + mrisdp_497 + mrisdp_498 + mrisdp_499 + 
               mrisdp_500 + mrisdp_501 + mrisdp_502 + mrisdp_503 + mrisdp_504 +
               mrisdp_505 + mrisdp_506 + mrisdp_507 + mrisdp_508 + mrisdp_509 + 
               mrisdp_510 + mrisdp_511 + mrisdp_512 +
               smri_vol_scs_cbwmatterlh + smri_vol_scs_ltventriclelh + smri_vol_scs_inflatventlh + 
               smri_vol_scs_crbwmatterlh + smri_vol_scs_crbcortexlh + smri_vol_scs_tplh + 
               smri_vol_scs_caudatelh + smri_vol_scs_putamenlh + smri_vol_scs_pallidumlh + 
               smri_vol_scs_3rdventricle + smri_vol_scs_4thventricle + smri_vol_scs_bstem + 
               smri_vol_scs_hpuslh + smri_vol_scs_amygdalalh + smri_vol_scs_csf + 
               smri_vol_scs_aal + smri_vol_scs_vedclh +
               smri_vol_scs_cbwmatterrh + smri_vol_scs_ltventriclerh + smri_vol_scs_inflatventrh + 
               smri_vol_scs_crbwmatterrh + smri_vol_scs_crbcortexrh + smri_vol_scs_tprh + 
               smri_vol_scs_caudaterh + smri_vol_scs_putamenrh + smri_vol_scs_pallidumrh + 
               smri_vol_scs_hpusrh + smri_vol_scs_amygdalarh + 
               smri_vol_scs_aar + smri_vol_scs_vedcrh + smri_vol_scs_wmhint + smri_vol_scs_wmhintlh + 
               smri_vol_scs_wmhintrh + smri_vol_scs_ccps + smri_vol_scs_ccmidps + 
               smri_vol_scs_ccct + smri_vol_scs_ccmidat + smri_vol_scs_ccat +
               smri_vol_scs_wholeb + smri_vol_scs_latventricles + smri_vol_scs_allventricles +
               smri_vol_scs_intracranialv + smri_vol_scs_suprateialv + smri_vol_scs_subcorticalgv,
             data = train_data)

summary(lm_model)
```

--------lm(bpm_internal ~ mri + demo)

lm_model <- lm(bpm_y_scr_internal_t ~ demo_sex_v2 + race_ethnicity + demo_brthdat_v2 +
               mrisdp_454 + mrisdp_455 + mrisdp_456 + mrisdp_457 + mrisdp_458 + mrisdp_459 + 
               mrisdp_460 + mrisdp_461 + mrisdp_462 + mrisdp_463 + mrisdp_464 +
               mrisdp_465 + mrisdp_466 + mrisdp_467 + mrisdp_468 + mrisdp_469 + 
               mrisdp_470 + mrisdp_471 + mrisdp_472 + mrisdp_473 + mrisdp_474 +
               mrisdp_475 + mrisdp_476 + mrisdp_477 + mrisdp_478 + mrisdp_479 + 
               mrisdp_480 + mrisdp_481 + mrisdp_482 + mrisdp_483 + mrisdp_484 +
               mrisdp_485 + mrisdp_486 + mrisdp_487 + mrisdp_488 + mrisdp_489 + 
               mrisdp_490 + mrisdp_491 + mrisdp_492 + mrisdp_493 + mrisdp_494 +
               mrisdp_495 + mrisdp_496 + mrisdp_497 + mrisdp_498 + mrisdp_499 + 
               mrisdp_500 + mrisdp_501 + mrisdp_502 + mrisdp_503 + mrisdp_504 +
               mrisdp_505 + mrisdp_506 + mrisdp_507 + mrisdp_508 + mrisdp_509 + 
               mrisdp_510 + mrisdp_511 + mrisdp_512 +
               smri_vol_scs_cbwmatterlh + smri_vol_scs_ltventriclelh + smri_vol_scs_inflatventlh + 
               smri_vol_scs_crbwmatterlh + smri_vol_scs_crbcortexlh + smri_vol_scs_tplh + 
               smri_vol_scs_caudatelh + smri_vol_scs_putamenlh + smri_vol_scs_pallidumlh + 
               smri_vol_scs_3rdventricle + smri_vol_scs_4thventricle + smri_vol_scs_bstem + 
               smri_vol_scs_hpuslh + smri_vol_scs_amygdalalh + smri_vol_scs_csf + 
               smri_vol_scs_aal + smri_vol_scs_vedclh +
               smri_vol_scs_cbwmatterrh + smri_vol_scs_ltventriclerh + smri_vol_scs_inflatventrh + 
               smri_vol_scs_crbwmatterrh + smri_vol_scs_crbcortexrh + smri_vol_scs_tprh + 
               smri_vol_scs_caudaterh + smri_vol_scs_putamenrh + smri_vol_scs_pallidumrh + 
               smri_vol_scs_hpusrh + smri_vol_scs_amygdalarh + 
               smri_vol_scs_aar + smri_vol_scs_vedcrh + smri_vol_scs_wmhint + smri_vol_scs_wmhintlh + 
               smri_vol_scs_wmhintrh + smri_vol_scs_ccps + smri_vol_scs_ccmidps + 
               smri_vol_scs_ccct + smri_vol_scs_ccmidat + smri_vol_scs_ccat +
               smri_vol_scs_wholeb + smri_vol_scs_latventricles + smri_vol_scs_allventricles +
               smri_vol_scs_intracranialv + smri_vol_scs_suprateialv + smri_vol_scs_subcorticalgv,
             data = train_data)

summary(lm_model)

--------


# XGBoost

Manual tuning resources
https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/#h-step-1-fix-learning-rate-and-number-of-estimators-for-tuning-tree-based-parameters
https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning#5.-Results-and-Conclusion-

Prepare dataset if demo is included: One hot encoding script in file called xgBoost_1.qmd

##xg(bpm_internal)

1. Initial Setup
```{r}
target <- "bpm_y_scr_internal_t"  
predictors <- colnames(full_dat)[!colnames(full_dat) %in% c("src_subject_id","cbcl_scr_syn_internal_t", "bpm_y_scr_external_t",       "bpm_y_scr_totalprob_t", "cbcl_scr_syn_external_t", "cbcl_scr_syn_totprob_t", target)]  

#print(predictors)
# Convert to matrix for XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(train_data[, predictors]), label = train_data[[target]])
dtest <- xgb.DMatrix(data = as.matrix(test_data[, predictors]), label = test_data[[target]])
```

2. Set initial values 

This will involve using XGBoostâ€™s xgb.cv function to determine the optimal number of trees first, and then applying grid search for tuning other parameters.

parameters we set in the meeting:
param_grid <- expand.grid(
nrounds = c(100, 300), # Number of boosting rounds
max_depth = c(2, 7, 15), # Maximum depth of a tree
eta = c(0.3, 0.1), # Learning rate
lambda = c(0.1, 1, 10) #regularization ====== Doesnt let me run lambda, not part of default xgboost package
)

eta = 0.3 nround best = 13
eta = 0.2 nround best = 18
eta = 0.1 nround best = 59
eta = 0.05 nround best = 152 or 213 or 52 why keep change even tho i set the seed 

```{r}
# Set initial parameters for the model
xgb_params <- list(
  objective = "reg:squarederror",                               # log loss function to minimize is set to regression with squared loss
  eval_metric = "rmse",                                         # primary eval_metric to guide model training
  eta = 0.05,                                                   # start high (0.1 works, but somewhere 0.05-0.3), will test other etas later
  max_depth = 5,                                                # Start with 5, to be tuned (somehwere 3-10)
  min_child_weight = 1,                                         # To be tuned: smaller value is chosen because it is a highly imbalanced class                                                                    problem, and leaf nodes can have smaller size groups
  subsample = 0.8,
  colsample_bytree = 0.8,                                       # These two : typical values range between 0.5-0.9.
  scale_pos_weight = 1,                                         # Because of high-class imbalance.
  seed = 1
)

# CV to find best nrounds
cv_model <- xgb.cv(
  params = xgb_params,
  data = dtrain,
  nrounds = 1000,
  nfold = 5,
  showsd = TRUE,
  stratified = FALSE,
  print_every_n = 10,
  early_stopping_rounds = 50,                                  # Stop early if no improvement
  maximize = FALSE
)

# print best nround
best_nrounds <- cv_model$best_iteration
print(paste("Optimal number of rounds: ", best_nrounds))

```
3. Tune tree specific parameters 

We tune max_depth and min_child_weight first because they have the highest impact on the model outcome. To start with, letâ€™s set wider ranges, and then we will perform another iteration for smaller ranges.

max_depth:
Controls the maximum depth of each tree. Increasing max_depth can help capture complex patterns but may also lead to over fitting. Lower values can prevent over fitting but might oversimplify the model.

min_child_weight:
Controls the minimum sum of instance weight needed in a child node. This parameter helps prevent over fitting by making it more difficult for leaf nodes to learn from small amounts of data.

We will tune them using a grid 
```{r}
# Setting up tuning grid
tune_grid <- expand.grid(
  nrounds = best_nrounds,
  max_depth = seq(2, 7, 15),
  min_child_weight = seq(1, 6, 2),
  eta = 0.1,
  gamma = 0,                                                # will tune later
  colsample_bytree = 0.8,                                   # will tune later
  subsample = 0.8                                           # will tune later
)

# TrainControl for caret
train_control <- trainControl(
  method = "cv",
  number = 5,
  verboseIter = TRUE
)

# Train using caret
caret_model <- train(
  x = train_data[, predictors, drop = FALSE],   # Ensure this returns a dataframe/matrix
  y = train_data[[target]],                     # This should return a vector
  method = "xgbTree",
  tuneGrid = tune_grid,
  trControl = train_control,
  metric = "RMSE"
)

# View best parameters
print(caret_model$bestTune)

```
4. Tune regularization parameters 

Once optimal max_depth and min_child_weight are found, tune gamma, subsample, and colsample_bytree.

```{r}
# Define new tuning grid
gamma_grid <- expand.grid(
  nrounds = best_nrounds,
  max_depth = caret_model$bestTune$max_depth,
  min_child_weight = caret_model$bestTune$min_child_weight,
  eta = 0.1,
  gamma = seq(0, 0.5, 0.1),
  colsample_bytree = 0.8,
  subsample = 0.8
)

# Train again
caret_model_2 <- train(
  x = train_data[, predictors, drop = FALSE],   # Ensure this returns a dataframe/matrix
  y = train_data[[target]],                     # This should return a vector
  method = "xgbTree",
  tuneGrid = gamma_grid,
  trControl = train_control,
  metric = "RMSE"
)

# Check the best parameters again
print(caret_model_2$bestTune)

```

5. Regularization and Final Model

Finally, you can fine-tune alpha and lambda for regularization.
```{r}
# Tuning regularization parameters
reg_grid <- expand.grid(
  nrounds = best_nrounds,
  max_depth = caret_model_2$bestTune$max_depth,
  min_child_weight = caret_model_2$bestTune$min_child_weight,
  eta = 0.1,
  gamma = caret_model_2$bestTune$gamma,
  subsample = caret_model_2$bestTune$subsample,
  colsample_bytree = caret_model_2$bestTune$colsample_bytree
)

caret_model_3 <- train(
  x = train_data[, predictors, drop = FALSE],   # Ensure this returns a dataframe/matrix
  y = train_data[[target]],                     # This should return a vector
  method = "xgbTree",
  tuneGrid = reg_grid,
  trControl = train_control,
  metric = "RMSE"
)

# Best parameters from all the tuning
print(caret_model_3$bestTune)

# Here, reduce learning rate and increase nrounds if needed

```


Post-training evaluate performance metrics:
```{r}
# Make predictions on test set
predictions <- predict(caret_model_3, newdata = test_data[, predictors])

################################### Post-training evaluate performance metrics

# Calculate MAE, R-squared, and correlation coefficient post-training
actual_values <- test_data[[target]]

# MAE calculation
mae <- mean(abs(predictions - actual_values))

# R-squared calculation
rsq <- cor(predictions, actual_values)^2

# Correlation coefficient
correlation <- cor(predictions, actual_values)

# RMSE calculation
rmse <- sqrt(mean((predictions - actual_values)^2))

# Display the results
cat("MAE: ", mae, "\n")
cat("R-squared: ", rsq, "\n")
cat("Correlation Coefficient: ", correlation, "\n")
cat("RMSE: ", rmse, "\n")
```


## xg(cbcl_internal)
```{r}
######################################## 1. Initial Setup
# define predictors and target
target <- "cbcl_scr_syn_internal_t" 
predictors <- colnames(full_dat)[!colnames(full_dat) %in% c("src_subject_id","bpm_y_scr_internal_t", "bpm_y_scr_external_t", "bpm_y_scr_totalprob_t", "cbcl_scr_syn_external_t", "cbcl_scr_syn_totprob_t", target)]  

#print(predictors)
# Convert to matrix for XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(train_data[, predictors]), label = train_data[[target]])
dtest <- xgb.DMatrix(data = as.matrix(test_data[, predictors]), label = test_data[[target]])

######################################## 2. Set initial values 

# initial parameters for model
xgb_params <- list(
  objective = "reg:squarederror",                              
  eval_metric = "rmse",                                       
  eta = 0.05,                                                 
  max_depth = 5,                                              
  min_child_weight = 1,                                                                                                 
  subsample = 0.8,
  colsample_bytree = 0.8,                                     
  scale_pos_weight = 1,                                        
  seed = 1
)

# CV to find best nrounds
cv_model <- xgb.cv(
  params = xgb_params,
  data = dtrain,
  nrounds = 1000,
  nfold = 5,
  showsd = TRUE,
  stratified = FALSE,
  print_every_n = 10,
  early_stopping_rounds = 50,                                  
  maximize = FALSE
)

# print best nround
best_nrounds <- cv_model$best_iteration
print(paste("Optimal number of rounds: ", best_nrounds))

######################################## 3. Tune tree specific parameters 

# Setting up tuning grid
tune_grid <- expand.grid(
  nrounds = best_nrounds,
  max_depth = seq(2, 7, 15),
  min_child_weight = seq(1, 6, 2),
  eta = 0.05,
  gamma = 0,                                                
  colsample_bytree = 0.8,                                   
  subsample = 0.8                                           
)

# TrainControl for caret
train_control <- trainControl(
  method = "cv",
  number = 5,
  verboseIter = TRUE
)

# Train using caret
caret_model <- train(
  x = train_data[, predictors, drop = FALSE],   
  y = train_data[[target]],                     
  method = "xgbTree",
  tuneGrid = tune_grid,
  trControl = train_control,
  metric = "RMSE"
)

# View best parameters
print(caret_model$bestTune)

###################################### 4. Tune regularization parameters 

# Define new tuning grid
gamma_grid <- expand.grid(
  nrounds = best_nrounds,
  max_depth = caret_model$bestTune$max_depth,
  min_child_weight = caret_model$bestTune$min_child_weight,
  eta = 0.05,
  gamma = seq(0, 0.5, 0.1),
  colsample_bytree = 0.8,
  subsample = 0.8
)

# Train again
caret_model_2 <- train(
  x = train_data[, predictors, drop = FALSE],  
  y = train_data[[target]],                    
  method = "xgbTree",
  tuneGrid = gamma_grid,
  trControl = train_control,
  metric = "RMSE"
)

# Check the best parameters again
print(caret_model_2$bestTune)

###################################  5. Regularization and Final Model

# Tuning regularization parameters
reg_grid <- expand.grid(
  nrounds = best_nrounds,
  max_depth = caret_model_2$bestTune$max_depth,
  min_child_weight = caret_model_2$bestTune$min_child_weight,
  eta = 0.05,
  gamma = caret_model_2$bestTune$gamma,
  subsample = caret_model_2$bestTune$subsample,
  colsample_bytree = caret_model_2$bestTune$colsample_bytree
)

caret_model_3 <- train(
  x = train_data[, predictors, drop = FALSE],   
  y = train_data[[target]],                     
  method = "xgbTree",
  tuneGrid = reg_grid,
  trControl = train_control,
  metric = "RMSE"
)

# Best parameters from all the tuning
print(caret_model_3$bestTune)

# Here, reduce learning rate and increase nrounds if needed
```


Post-training evaluate performance metrics:
```{r}
# Make predictions on test set
predictions <- predict(caret_model_3, newdata = test_data[, predictors])

################################### Post-training evaluate performance metrics

# Calculate MAE, R-squared, and correlation coefficient post-training
actual_values <- test_data[[target]]

# MAE calculation
mae <- mean(abs(predictions - actual_values))

# R-squared calculation
rsq <- cor(predictions, actual_values)^2

# Correlation coefficient
correlation <- cor(predictions, actual_values)

# RMSE calculation
rmse <- sqrt(mean((predictions - actual_values)^2))

# Display the results
cat("MAE: ", mae, "\n")
cat("R-squared: ", rsq, "\n")
cat("Correlation Coefficient: ", correlation, "\n")
cat("RMSE: ", rmse, "\n")
```



## xg(bmp_external)
```{r}
######################################## 1. Initial Setup
# define predictors and target
target <- "bpm_y_scr_external_t" 
predictors <- colnames(full_dat)[!colnames(full_dat) %in% c("src_subject_id","bpm_y_scr_internal_t", "cbcl_scr_syn_internal_t",       "bpm_y_scr_totalprob_t", "cbcl_scr_syn_external_t", "cbcl_scr_syn_totprob_t", target)]  

#print(predictors)
# Convert to matrix for XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(train_data[, predictors]), label = train_data[[target]])
dtest <- xgb.DMatrix(data = as.matrix(test_data[, predictors]), label = test_data[[target]])

######################################## 2. Set initial values 

# initial parameters for model
xgb_params <- list(
  objective = "reg:squarederror",                              
  eval_metric = "rmse",                                       
  eta = 0.05,                                                 
  max_depth = 5,                                              
  min_child_weight = 1,                                                                                                 
  subsample = 0.8,
  colsample_bytree = 0.8,                                     
  scale_pos_weight = 1,                                        
  seed = 1
)

# CV to find best nrounds
cv_model <- xgb.cv(
  params = xgb_params,
  data = dtrain,
  nrounds = 1000,
  nfold = 5,
  showsd = TRUE,
  stratified = FALSE,
  print_every_n = 10,
  early_stopping_rounds = 50,                                  
  maximize = FALSE
)

# print best nround
best_nrounds <- cv_model$best_iteration
print(paste("Optimal number of rounds: ", best_nrounds))

######################################## 3. Tune tree specific parameters 

# Setting up tuning grid
tune_grid <- expand.grid(
  nrounds = best_nrounds,
  max_depth = seq(2, 7, 15),
  min_child_weight = seq(1, 6, 2),
  eta = 0.05,
  gamma = 0,                                                
  colsample_bytree = 0.8,                                   
  subsample = 0.8                                           
)

# TrainControl for caret
train_control <- trainControl(
  method = "cv",
  number = 5,
  verboseIter = TRUE
)

# Train using caret
caret_model <- train(
  x = train_data[, predictors, drop = FALSE],   
  y = train_data[[target]],                     
  method = "xgbTree",
  tuneGrid = tune_grid,
  trControl = train_control,
  metric = "RMSE"
)

# View best parameters
print(caret_model$bestTune)

###################################### 4. Tune regularization parameters 

# Define new tuning grid
gamma_grid <- expand.grid(
  nrounds = best_nrounds,
  max_depth = caret_model$bestTune$max_depth,
  min_child_weight = caret_model$bestTune$min_child_weight,
  eta = 0.05,
  gamma = seq(0, 0.5, 0.1),
  colsample_bytree = 0.8,
  subsample = 0.8
)

# Train again
caret_model_2 <- train(
  x = train_data[, predictors, drop = FALSE],  
  y = train_data[[target]],                    
  method = "xgbTree",
  tuneGrid = gamma_grid,
  trControl = train_control,
  metric = "RMSE"
)

# Check the best parameters again
print(caret_model_2$bestTune)

###################################  5. Regularization and Final Model

# Tuning regularization parameters
reg_grid <- expand.grid(
  nrounds = best_nrounds,
  max_depth = caret_model_2$bestTune$max_depth,
  min_child_weight = caret_model_2$bestTune$min_child_weight,
  eta = 0.05,
  gamma = caret_model_2$bestTune$gamma,
  subsample = caret_model_2$bestTune$subsample,
  colsample_bytree = caret_model_2$bestTune$colsample_bytree
)

caret_model_3 <- train(
  x = train_data[, predictors, drop = FALSE],   
  y = train_data[[target]],                     
  method = "xgbTree",
  tuneGrid = reg_grid,
  trControl = train_control,
  metric = "RMSE"
)

# Best parameters from all the tuning
print(caret_model_3$bestTune)

# Here, reduce learning rate and increase nrounds if needed
```

Post-training evaluate performance metrics:
```{r}
# Make predictions on test set
predictions <- predict(caret_model_3, newdata = test_data[, predictors])

################################### Post-training evaluate performance metrics

# Calculate MAE, R-squared, and correlation coefficient post-training
actual_values <- test_data[[target]]

# MAE calculation
mae <- mean(abs(predictions - actual_values))

# R-squared calculation
rsq <- cor(predictions, actual_values)^2

# Correlation coefficient
correlation <- cor(predictions, actual_values)

# RMSE calculation
rmse <- sqrt(mean((predictions - actual_values)^2))

# Display the results
cat("MAE: ", mae, "\n")
cat("R-squared: ", rsq, "\n")
cat("Correlation Coefficient: ", correlation, "\n")
cat("RMSE: ", rmse, "\n")
```


## xg(cbcl_external)
```{r}
######################################## 1. Initial Setup
# define predictors and target
target <- "cbcl_scr_syn_external_t" 
predictors <- colnames(full_dat)[!colnames(full_dat) %in% c("src_subject_id","bpm_y_scr_internal_t", "bpm_y_scr_external_t",       "bpm_y_scr_totalprob_t", "cbcl_scr_syn_internal_t", "cbcl_scr_syn_totprob_t", target)]  

#print(predictors)
# Convert to matrix for XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(train_data[, predictors]), label = train_data[[target]])
dtest <- xgb.DMatrix(data = as.matrix(test_data[, predictors]), label = test_data[[target]])

######################################## 2. Set initial values 

# initial parameters for model
xgb_params <- list(
  objective = "reg:squarederror",                              
  eval_metric = "rmse",                                       
  eta = 0.05,                                                 
  max_depth = 5,                                              
  min_child_weight = 1,                                                                                                 
  subsample = 0.8,
  colsample_bytree = 0.8,                                     
  scale_pos_weight = 1,                                        
  seed = 1
)

# CV to find best nrounds
cv_model <- xgb.cv(
  params = xgb_params,
  data = dtrain,
  nrounds = 1000,
  nfold = 5,
  showsd = TRUE,
  stratified = FALSE,
  print_every_n = 10,
  early_stopping_rounds = 50,                                  
  maximize = FALSE
)

# print best nround
best_nrounds <- cv_model$best_iteration
print(paste("Optimal number of rounds: ", best_nrounds))

######################################## 3. Tune tree specific parameters 

# Setting up tuning grid
tune_grid <- expand.grid(
  nrounds = best_nrounds,
  max_depth = seq(2, 7, 15),
  min_child_weight = seq(1, 6, 2),
  eta = 0.05,
  gamma = 0,                                                
  colsample_bytree = 0.8,                                   
  subsample = 0.8                                           
)

# TrainControl for caret
train_control <- trainControl(
  method = "cv",
  number = 5,
  verboseIter = TRUE
)

# Train using caret
caret_model <- train(
  x = train_data[, predictors, drop = FALSE],   
  y = train_data[[target]],                     
  method = "xgbTree",
  tuneGrid = tune_grid,
  trControl = train_control,
  metric = "RMSE"
)

# View best parameters
print(caret_model$bestTune)

###################################### 4. Tune regularization parameters 

# Define new tuning grid
gamma_grid <- expand.grid(
  nrounds = best_nrounds,
  max_depth = caret_model$bestTune$max_depth,
  min_child_weight = caret_model$bestTune$min_child_weight,
  eta = 0.05,
  gamma = seq(0, 0.5, 0.1),
  colsample_bytree = 0.8,
  subsample = 0.8
)

# Train again
caret_model_2 <- train(
  x = train_data[, predictors, drop = FALSE],  
  y = train_data[[target]],                    
  method = "xgbTree",
  tuneGrid = gamma_grid,
  trControl = train_control,
  metric = "RMSE"
)

# Check the best parameters again
print(caret_model_2$bestTune)

###################################  5. Regularization and Final Model

# Tuning regularization parameters
reg_grid <- expand.grid(
  nrounds = best_nrounds,
  max_depth = caret_model_2$bestTune$max_depth,
  min_child_weight = caret_model_2$bestTune$min_child_weight,
  eta = 0.05,
  gamma = caret_model_2$bestTune$gamma,
  subsample = caret_model_2$bestTune$subsample,
  colsample_bytree = caret_model_2$bestTune$colsample_bytree
)

caret_model_3 <- train(
  x = train_data[, predictors, drop = FALSE],   
  y = train_data[[target]],                     
  method = "xgbTree",
  tuneGrid = reg_grid,
  trControl = train_control,
  metric = "RMSE"
)

# Best parameters from all the tuning
print(caret_model_3$bestTune)

# Here, reduce learning rate and increase nrounds if needed
```

Post-training evaluate performance metrics:
```{r}
# Make predictions on test set
predictions <- predict(caret_model_3, newdata = test_data[, predictors])

################################### Post-training evaluate performance metrics

# Calculate MAE, R-squared, and correlation coefficient post-training
actual_values <- test_data[[target]]

# MAE calculation
mae <- mean(abs(predictions - actual_values))

# R-squared calculation
rsq <- cor(predictions, actual_values)^2

# Correlation coefficient
correlation <- cor(predictions, actual_values)

# RMSE calculation
rmse <- sqrt(mean((predictions - actual_values)^2))

# Display the results
cat("MAE: ", mae, "\n")
cat("R-squared: ", rsq, "\n")
cat("Correlation Coefficient: ", correlation, "\n")
cat("RMSE: ", rmse, "\n")
```

```{r}

# Train XGBoost model with evaluation metric monitoring
watchlist <- list(train = dtrain, eval = dtest)

xgb_params <- list(
  objective = "reg:squarederror",
  eval_metric = "rmse",
  eta = 0.05,
  max_depth = 5,
  subsample = 0.8,
  colsample_bytree = 0.8
)

model <- xgb.train(
  params = xgb_params,
  data = dtrain,
  nrounds = 100,
  watchlist = watchlist,
  print_every_n = 10,
  early_stopping_rounds = 10
)

# Access the evaluation log
eval_log <- model$evaluation_log

# Convert to long format for ggplot2
eval_log_long <- reshape2::melt(eval_log, id.vars = "iter")

# Generate RMSE plot
ggplot(eval_log_long, aes(x = iter, y = value, color = variable)) +
  geom_line() +
  labs(title = "RMSE over iterations", x = "Number of iterations", y = "RMSE") +
  theme_minimal() +
  scale_color_manual(values = c("train_rmse" = "blue", "test_rmse" = "red"))

```


```{r}

```



```{r}

```


```{r}

```


```{r}

```




```{r}

```


```{r}

```


```{r}

```




```{r}

```


```{r}

```


```{r}

```




